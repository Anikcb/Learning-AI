## Chain Rule
- [Chain Rule of Differentiation with BackPropagation](https://youtu.be/CRB266Eyjkg?si=stZF3GSjHokT2o7K)
- [Chain Rule | Deep Learning Tutorial](https://youtu.be/5ogmEkujoqE?si=CCKQSTUZAG1vJmpU)
  
## Activation Function
- [Activation Functions - EXPLAINED!](https://youtu.be/s-V7gKrsels?si=Zcq3uXQly8UGY72L)
- [Activation function in Neural Network](https://youtu.be/Y9qdKsOHRjA?si=BiyYFfb8DZpGE9O-)
- [Sigmoid Function](https://youtu.be/TPqr8t919YM?si=tqCBV_SILpkEgMNq)
## Gradient Decent
Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a [convex Function](https://youtu.be/7QmGj1_i3MU?si=orUYsv2TKb8tUs54) and tweaks its parameters iteratively to minimize a given function to its local minimum.
- [Vanishing Gradient Problem](https://www.engati.com/glossary/vanishing-gradient-problem#:~:text=Vanishing%20gradient%20problem%20is%20a,layers%20to%20the%20earlier%20layers.)

