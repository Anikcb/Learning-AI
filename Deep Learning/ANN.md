## Basic Flow of Neural Network
This course didn't explain in depth knowledge but the basic flow of Neural network and some topics to understand NN explained prefectly
1. [How does Neural Network Work](https://youtu.be/JkeiEYkLEvM?si=6UqWlhp7bqfjKxVi)
2. [Activation Functions Part-1](https://youtu.be/SXrXUqDjICA?si=EU00oLzaSSumEP0j)
3. [How to train Neural Network with BackPropogation](https://youtu.be/mH9GBJ6og5A?si=IScBaTaQlGctcD38)
4. [How to train MultiLayer Neural Network and Gradient Descent](https://www.youtube.com/watch?v=cxPAvoIbsIk&list=PLZoTAELRMXVPGU70ZGsckrMdr0FteeRUi&index=9&ab_channel=KrishNaik)

## Chain Rule
- [Chain Rule of Differentiation with BackPropagation](https://youtu.be/CRB266Eyjkg?si=stZF3GSjHokT2o7K)
- [Chain Rule Deep Learning Tutorial](https://youtu.be/5ogmEkujoqE?si=CCKQSTUZAG1vJmpU)
- [The Chain Rule of Calculus](https://medium.com/@ppuneeth73/the-chain-rule-of-calculus-the-backbone-of-deep-learning-backpropagation-9d35affc05e7)
  
## Activation Function
- [Activation Functions - EXPLAINED!](https://youtu.be/s-V7gKrsels?si=Zcq3uXQly8UGY72L)
- [Activation Functions in Deep Learning | Sigmoid, Tanh and Relu Activation Function](https://www.youtube.com/watch?v=7LcUkgzx3AY&t=87s&ab_channel=CampusX)
- [Activation function in Neural Network](https://youtu.be/Y9qdKsOHRjA?si=BiyYFfb8DZpGE9O-)
- [Sigmoid Function](https://youtu.be/TPqr8t919YM?si=tqCBV_SILpkEgMNq)
- [Relu Variants Explained | Leaky Relu | Parametric Relu | Elu | Selu ](https://www.youtube.com/watch?v=2OwWs7Hzr9g&ab_channel=CampusX)
  
## Gradient Decent
Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a [convex Function](https://youtu.be/7QmGj1_i3MU?si=orUYsv2TKb8tUs54) and tweaks its parameters iteratively to minimize a given function to its local minimum.
- [Gradient Descent in Neural Networks](https://www.youtube.com/watch?v=7z6yXpYk7sw&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=21&ab_channel=CampusX)
- [Vanishing Gradient Problem](https://www.youtube.com/watch?v=JIWXbzRXk1I&list=PLZoTAELRMXVPGU70ZGsckrMdr0FteeRUi&index=11)
- [Vanishing Gradient Problem in ANN](https://www.youtube.com/watch?v=uCrevbBh0zM&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=20&ab_channel=CampusX)
- [Vanishing Gradient](https://www.engati.com/glossary/vanishing-gradient-problem#:~:text=Vanishing%20gradient%20problem%20is%20a,layers%20to%20the%20earlier%20layers.)
### Optimzation in Gradient Decent [[youtube playlist](https://youtube.com/playlist?list=PLKnIA16_RmvYhD5pqAeVu3j_jTjnTJIW2&si=VxjOrf-l0BH5jqxL)]
- [Adam Optimizer Explained in Detail with Animations](https://www.youtube.com/watch?v=N5AynalXD9g&ab_channel=CampusX)
  
## Backpropagation
- [Backpropagation in Deep Learning | Part 1](https://www.youtube.com/watch?v=6M1wWQmcUjQ&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=16&ab_channel=CampusX)
- [Backpropagation in Deep Learning | Part 2](https://www.youtube.com/watch?v=ma6hWrU-LaI&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=17&ab_channel=CampusX)
- [Backpropagation in Deep Learning | Part 3](https://www.youtube.com/watch?v=6xO-x8y0YSY&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=18&ab_channel=CampusX)
- [Understanding Backpropagation Algorithm](https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd)
