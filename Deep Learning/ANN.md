## Basic Flow of Neural Network
This course didn't explain in depth knowledge but the basic flow of Neural network and some topics to understand NN explained prefectly
1. [How does Neural Network Work](https://youtu.be/JkeiEYkLEvM?si=6UqWlhp7bqfjKxVi)
2. [Activation Functions Part-1](https://youtu.be/SXrXUqDjICA?si=EU00oLzaSSumEP0j)
3. [How to train Neural Network with BackPropogation](https://youtu.be/mH9GBJ6og5A?si=IScBaTaQlGctcD38)
4. [How to train MultiLayer Neural Network and Gradient Descent](https://www.youtube.com/watch?v=cxPAvoIbsIk&list=PLZoTAELRMXVPGU70ZGsckrMdr0FteeRUi&index=9&ab_channel=KrishNaik)

## Chain Rule
- [Chain Rule of Differentiation with BackPropagation](https://youtu.be/CRB266Eyjkg?si=stZF3GSjHokT2o7K)
- [Chain Rule Deep Learning Tutorial](https://youtu.be/5ogmEkujoqE?si=CCKQSTUZAG1vJmpU)
- [The Chain Rule of Calculus](https://medium.com/@ppuneeth73/the-chain-rule-of-calculus-the-backbone-of-deep-learning-backpropagation-9d35affc05e7)
  
## Activation Function
- [Activation Functions - EXPLAINED!](https://youtu.be/s-V7gKrsels?si=Zcq3uXQly8UGY72L)
- [Activation function in Neural Network](https://youtu.be/Y9qdKsOHRjA?si=BiyYFfb8DZpGE9O-)
- [Sigmoid Function](https://youtu.be/TPqr8t919YM?si=tqCBV_SILpkEgMNq)
  
## Gradient Decent
Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a [convex Function](https://youtu.be/7QmGj1_i3MU?si=orUYsv2TKb8tUs54) and tweaks its parameters iteratively to minimize a given function to its local minimum.
- [Vanishing Gradient Problem](https://www.engati.com/glossary/vanishing-gradient-problem#:~:text=Vanishing%20gradient%20problem%20is%20a,layers%20to%20the%20earlier%20layers.)

