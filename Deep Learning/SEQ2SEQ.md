### `Encoder` &rarr; `Attention` &rarr; `Transformer`
***
### Sequence-to-Sequence Architecture
- [Encoder Decoder](https://www.youtube.com/watch?v=KiL74WsgxoA&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=68)
### Attention Mechanism
- [Attention Mechanism in 1 video](https://www.youtube.com/watch?v=rj5V6q6-XUM&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=69&ab_channel=CampusX)
- [Bahdanau Attention Vs Luong Attention](https://www.youtube.com/watch?v=0hZT4_fHfNQ&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=71)
### Transformers
- [Introduction to Transformers](https://www.youtube.com/watch?v=BjRVS2wTtcA&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=72&ab_channel=CampusX)
<img src="https://github.com/Anikcb/Learning-AI/blob/main/Readme%20Images/transformer.png?raw=true"  width="70%" height="70%">

### Self Attention
 - [What is self Attention](https://www.youtube.com/watch?v=XnGGmvpDLA0&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=73&ab_channel=CampusX)
 - [Self Attention in Transformers](https://www.youtube.com/watch?v=-tCKPl_8Xb8&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=73&ab_channel=CampusX)
 - [Scaled Dot Product Attention](https://www.youtube.com/watch?v=r7mAt0iVqwo&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=74&ab_channel=CampusX)
 - [What is Multi-head Attention in Transformers](https://www.youtube.com/watch?v=bX2QwpjsmuA&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=78&ab_channel=CampusX)
 - [Positional Encoding in Transformers](https://www.youtube.com/watch?v=GeoQBNNqIbM&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=78&ab_channel=CampusX)
 - [Layer Normalization in Transformers](https://www.youtube.com/watch?v=qti0QPdaelg&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=79&ab_channel=CampusX)
