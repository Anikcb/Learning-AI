### Transformers
- [Introduction to Transformers](https://www.youtube.com/watch?v=BjRVS2wTtcA&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=72&ab_channel=CampusX)
<img src="https://github.com/Anikcb/Learning-AI/blob/main/Readme%20Images/transformer.png?raw=true"  width="70%" height="70%">

### Self Attention
 - [What is self Attention](https://www.youtube.com/watch?v=XnGGmvpDLA0&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=73&ab_channel=CampusX)
 - [Self Attention in Transformers](https://www.youtube.com/watch?v=-tCKPl_8Xb8&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=73&ab_channel=CampusX)
 - [Scaled Dot Product Attention](https://www.youtube.com/watch?v=r7mAt0iVqwo&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=74&ab_channel=CampusX)
 - [What is Multi-head Attention in Transformers](https://www.youtube.com/watch?v=bX2QwpjsmuA&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=78&ab_channel=CampusX)
    #### [Multihead Visualization](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ#scrollTo=-QnRteSLP0Hm)
 - [Positional Encoding in Transformers](https://www.youtube.com/watch?v=GeoQBNNqIbM&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=78&ab_channel=CampusX)
    #### [Linear Relationships in the Transformerâ€™s Positional Encoding](https://blog.timodenk.com/linear-relationships-in-the-transformers-positional-encoding/)
 - [Layer Normalization in Transformers](https://www.youtube.com/watch?v=qti0QPdaelg&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=79&ab_channel=CampusX)
 - [Transformer Architecture](https://www.youtube.com/watch?v=m6onaKFzF94&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=81)
