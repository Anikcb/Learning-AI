### Transformers
- [Introduction to Transformers](https://www.youtube.com/watch?v=BjRVS2wTtcA&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=72&ab_channel=CampusX)
<img src="https://github.com/Anikcb/Learning-AI/blob/main/Readme%20Images/transformer.png?raw=true"  width="70%" height="70%">

### Self Attention
 - [What is self Attention](https://www.youtube.com/watch?v=XnGGmvpDLA0&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=73&ab_channel=CampusX)
 - [Self Attention in Transformers](https://www.youtube.com/watch?v=-tCKPl_8Xb8&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=73&ab_channel=CampusX)
 - [Scaled Dot Product Attention](https://www.youtube.com/watch?v=r7mAt0iVqwo&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=74&ab_channel=CampusX)
 - [What is Multi-head Attention in Transformers](https://www.youtube.com/watch?v=bX2QwpjsmuA&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=78&ab_channel=CampusX)
 - [Positional Encoding in Transformers](https://www.youtube.com/watch?v=GeoQBNNqIbM&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=78&ab_channel=CampusX)
 - [Layer Normalization in Transformers](https://www.youtube.com/watch?v=qti0QPdaelg&list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn&index=79&ab_channel=CampusX)
